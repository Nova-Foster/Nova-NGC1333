{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Heat map of difference in first and second component? Use abs difference and see if there is anything\n",
    "- Look into l vs d component plot. Is this showing the rotation or have I found something unkown?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "#import aplpy as apl\n",
    "#import matplotlib.colors as col\n",
    "import matplotlib.pyplot as plt\n",
    "import spectral_cube as sc\n",
    "#from matplotlib.colorbar import ColorbarBase\n",
    "from astropy.wcs import WCS\n",
    "from astropy.io import fits\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2ch(v, w): # convert velocity (km/s) to channel \n",
    "    x_tempo, y_tempo, v_tempo   = w.wcs_pix2world(0, 0, 0, 0)\n",
    "    x_ch, y_ch, v_ch   = w.wcs_world2pix(x_tempo, y_tempo, v*1000.0, 0)\n",
    "    v_ch = int(round(float(v_ch), 0))\n",
    "    return v_ch\n",
    "\n",
    "def ch2v(ch, w):#km/s\n",
    "    x, y, v   = w.wcs_pix2world(0, 0, ch, 0)\n",
    "    return v/1000.0\n",
    "\n",
    "def del_header_key(header, keys): # delete header key\n",
    "    import copy\n",
    "    h = copy.deepcopy(header)\n",
    "    for k in keys:\n",
    "        try:\n",
    "            del h[k]\n",
    "        except:\n",
    "            pass\n",
    "    return h\n",
    "\n",
    "def make_new_hdu_integ(hdu, v_start_wcs, v_end_wcs, w): # make header of integrated intensity map\n",
    "    data = hdu.data\n",
    "    header = hdu.header\n",
    "    start_ch, end_ch = v2ch(v_start_wcs, w), v2ch(v_end_wcs, w)\n",
    "    new_data = np.sum(data[start_ch:end_ch+1], axis=0)*np.abs(header[\"CDELT3\"])/1000.0\n",
    "    header = del_header_key(header, [\"CRVAL3\", \"CRPIX3\", \"CRVAL3\", \"CDELT3\", \"CUNIT3\", \"CTYPE3\", \"CROTA3\", \"NAXIS3\", \"PC1_3\", \"PC2_3\", \"PC3_3\", \"PC3_1\", \"PC3_2\"])\n",
    "    header[\"NAXIS\"] = 2\n",
    "    new_hdu = fits.PrimaryHDU(new_data, header)\n",
    "    return new_hdu\n",
    "\n",
    "def make_new_hdu_integ_ch(hdu, v_start_ch, v_end_ch, w): # make header of integrated intensity map\n",
    "    data = hdu.data\n",
    "    header = hdu.header\n",
    "    new_data = np.sum(data[v_start_ch:v_end_ch], axis=0)*header[\"CDELT3\"]/1000.0\n",
    "    header = del_header_key(header, [\"CRVAL3\", \"CRPIX3\", \"CRVAL3\", \"CDELT3\", \"CUNIT3\", \"CTYPE3\", \"CROTA3\", \"NAXIS3\", \"PC1_3\", \"PC2_3\", \"PC3_3\", \"PC3_1\", \"PC3_2\"])\n",
    "    header[\"NAXIS\"] = 2\n",
    "    new_hdu = fits.PrimaryHDU(new_data, header)\n",
    "    return new_hdu\n",
    "\n",
    "def new_header_v_decomp(original_hdu,max_fitted,val_type='unkown',val_unit='unkown'):\n",
    "    header = original_hdu.header\n",
    "    header[\"BTYPE\"] = val_type\n",
    "    header[\"BUNIT\"] = val_unit\n",
    "    #header[\"CTYPE1\"] = val_type\n",
    "    header[\"CUNIT3\"] = val_unit\n",
    "    header[\"NAXIS3\"] = max_fitted\n",
    "    header[\"CRVAL3\"] = 1\n",
    "    header[\"CDELT3\"] = 1\n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = '13CO'\n",
    "filename = f\"C:\\\\Users\\\\alexf\\Japan internship\\\\{species}\\\\ngc1333TP.{species}.cube.valueK.fits\"\n",
    "hdu = fits.open(filename)[0]\n",
    "original_header = hdu.header\n",
    "w = WCS(hdu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\fitparams_chunk_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load fit params intially to find number of fits per pixel\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     temp \u001b[38;5;241m=\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43malexf\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mNova-NGC1333\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mNova-NGC1333\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mfitparams_chunk_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m         dec_cont\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(temp)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alexf\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\fitparams_chunk_0.npy'"
     ]
    }
   ],
   "source": [
    "temp_params = []\n",
    "temp_errors = []\n",
    "temp_residuals = []\n",
    "temp_model = []\n",
    "temp_threshold = []\n",
    "dec_cont=0\n",
    "fit_count = np.ones((860,560)) * np.nan\n",
    "\n",
    "# Load fit params intially to find number of fits per pixel\n",
    "for num in range(6):\n",
    "    temp = (np.load(f'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\fitparams_chunk_{num}.npy',allow_pickle=True))\n",
    "    if num!=0:\n",
    "        dec_cont+= np.shape(temp)[0]\n",
    "\n",
    "    print(dec_cont)\n",
    "    for i in range(np.shape(temp)[0]):\n",
    "        for j in range(np.shape(temp)[1]):\n",
    "            fit_count[i+dec_cont,j] = np.shape(temp[i,j])[0] - 1\n",
    "del temp                  # delete temp array\n",
    "\n",
    "# Load each processed chunk and append arrays into one list\n",
    "for num in range(6):\n",
    "    temp_params.append (np.load(f'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\fitparams_chunk_{num}.npy',allow_pickle=True))\n",
    "    temp_errors.append (np.load(f'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\fiterrors_chunk_{num}.npy',allow_pickle=True))\n",
    "    #temp_residuals.append (np.load(f'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\residuals_chunk_{num}.npy',allow_pickle=True))\n",
    "    temp_threshold.append(np.load(f'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\threshold_{num}.npy',allow_pickle=True))\n",
    "    temp_model.append(np.load(f'C:\\\\Users\\\\alexf\\\\Nova-NGC1333\\\\Nova-NGC1333\\\\output\\\\model_chunk_{num}.npy',allow_pickle=True))\n",
    "\n",
    "# Combine lists into one large array of correct size, delete temp arrays after for memory\n",
    "stack_param = np.vstack(temp_params)\n",
    "stack_errors = np.vstack(temp_errors)\n",
    "#stack_residuals = np.vstack(temp_residuals)                               # Residuals is now created in this file instead of loaded in\n",
    "stack_model = np.vstack(temp_model)\n",
    "stack_threshold = np.vstack(temp_threshold)\n",
    "del temp_params, temp_errors, temp_residuals, temp_threshold, temp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating arrays...\n",
      "Execution time: 1.59 minutes\n",
      "Assigning values...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "Execution time: 14.76 minutes\n",
      "Creating residuals...\n",
      "Execution time: 0.25 minutes\n",
      "Replacing placeholder with nan...\n",
      "Execution time: 0.03 minutes\n",
      "Stacking values for saving...\n",
      "Execution time: 0.00 minutes\n",
      "Creating parameter cubes and saving...\n",
      "Execution time: 0.03 minutes\n",
      "Creating residual cube, model cube and saving...\n",
      "Execution time: 1.06 minutes\n",
      "Saving number of gaussian and threshold plot...\n",
      "Execution time: 0.00 minutes\n",
      "Done :)\n"
     ]
    }
   ],
   "source": [
    "max_fitted = int(np.nanmax(fit_count))\n",
    "\n",
    "# Actual arrays to be used. Shaped dec X ra X fit number\n",
    "print('Creating arrays...')\n",
    "start_time = time.time()\n",
    "fit_velocity = np.ones((860,560,max_fitted),dtype=np.float32) * np.nan\n",
    "fit_scaling = np.ones((860,560,max_fitted),dtype=np.float32) * np.nan\n",
    "fit_dispersion = np.ones((860,560,max_fitted),dtype=np.float32) * np.nan\n",
    "err_velocity = np.ones((860,560,max_fitted),dtype=np.float32) * np.nan\n",
    "err_scaling = np.ones((860,560,max_fitted),dtype=np.float32) * np.nan\n",
    "err_dispersion = np.ones((860,560,max_fitted),dtype=np.float32) * np.nan\n",
    "model = np.ones((860,560,max_fitted,len(stack_model[0,0][0])),dtype=np.float16) * np.nan\n",
    "#final_residuals = np.ones((860,560,len(stack_model[0,0][0])),dtype=np.float32) * np.nan\n",
    "\n",
    "final_model = np.ones((860,560,len(stack_model[0,0][0])),dtype=np.float32) * np.nan\n",
    "threshold = np.ones((860,560,max_fitted)) * np.nan\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "\n",
    "\n",
    "print('Assigning values...')\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each ra and dec combination then each fit for that positions\n",
    "# Doing it this way gets around any problems form inhomogenous arrays as that was needed based on not knowing how many fits would be needed\n",
    "for i in range(860):\n",
    "    print(i)\n",
    "    for j in range(560):\n",
    "        #final_residuals[i,j] = stack_residuals[i,j]\n",
    "        threshold[i,j] = stack_threshold[i,j]\n",
    "        for k in range(max_fitted):\n",
    "            try:\n",
    "                fit_scaling[i,j,k] = stack_param[i,j][k][0]\n",
    "                fit_velocity[i,j,k] = stack_param[i,j][k][1]\n",
    "                fit_dispersion[i,j,k] = stack_param[i,j][k][2]\n",
    "                \n",
    "                err_scaling[i,j,k] = stack_errors[i,j][k][0]\n",
    "                err_velocity[i,j,k] = stack_errors[i,j][k][1]\n",
    "                err_dispersion[i,j,k] = stack_errors[i,j][k][2]\n",
    "                \n",
    "                model[i,j,k] = stack_model[i,j][k]\n",
    "                \n",
    "            except IndexError:\n",
    "                # Looping up to the max number of fits, not all of them have an index that high\n",
    "                continue\n",
    "            \n",
    "            finally:\n",
    "                # Create the model data. temp_model is only used here to make the code a bit easier to follow\n",
    "                model[i,j][model[i,j]==-320]=0\n",
    "                # Sum each fitted gaussian together\n",
    "                final_model[i,j] = np.nansum(model[i,j],axis=0)\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "\n",
    "# Transpose the array so that it matches the ALMA data (channel x dec x ra)\n",
    "print('Creating residuals...')\n",
    "start_time = time.time()\n",
    "final_model = np.transpose(final_model,axes=[2,0,1])\n",
    "final_residuals = hdu.data- final_model\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "\n",
    "\n",
    "\n",
    "print('Replacing placeholder with nan...') \n",
    "# Need to do it like this due to inhomogenous data types earlier. Python needs those to be 'pickled' to save them but that can corrupt data if nan values are present\n",
    "# -320 is the placeholder set before saving so this just swaps it back\n",
    "start_time = time.time()\n",
    "fit_scaling[fit_scaling==-320]=np.nan  \n",
    "fit_velocity[fit_velocity==-320]=np.nan\n",
    "fit_dispersion[fit_dispersion==-320]=np.nan\n",
    "\n",
    "err_scaling[err_scaling==-320]=np.nan\n",
    "err_velocity[err_velocity==-320]=np.nan\n",
    "err_dispersion[err_dispersion==-320]=np.nan\n",
    "\n",
    "final_residuals[final_residuals==-320]=np.nan\n",
    "threshold[threshold==-320]=np.nan \n",
    "#model[model==-320]=0\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "\n",
    "\n",
    "\n",
    "print('Stacking values for saving...')\n",
    "start_time = time.time()\n",
    "stacked_total_vals = np.stack([fit_scaling,fit_velocity,fit_dispersion,err_scaling,err_velocity,err_dispersion])\n",
    "del fit_scaling, fit_velocity, fit_dispersion, err_scaling, err_velocity, err_dispersion\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "\n",
    "\n",
    "print('Creating parameter cubes and saving...')\n",
    "# Loop for each value type and again for each fit type. Create a new header with the correct wcs and dimensions and then save it as a cube\n",
    "start_time = time.time()\n",
    "fit_value = ['Scaling','Velocity','Dispersion']\n",
    "fit_type = ['Fit','Error']\n",
    "chan_num = '10chan'\n",
    "count=0\n",
    "for types in fit_type:\n",
    "    for values in fit_value:\n",
    "        current_data = stacked_total_vals[count]\n",
    "        new_header = new_header_v_decomp(hdu,max_fitted,val_type=f\"{values}\",val_unit=\"km/s\")\n",
    "        trans = np.transpose(current_data,axes=[2,0,1])\n",
    "        new_cube = sc.SpectralCube(data=trans,header=new_header,wcs=w)\n",
    "        new_cube.write(f'V_decomp//C18O//{species}_{types}_{values}.{chan_num}.cube.fits',format='fits',overwrite=True)\n",
    "        count+=1\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "del new_cube, stack_errors, stack_param, stacked_total_vals, fit_value, fit_type    # Delete variables now they are saved as large cubes can be expensive\n",
    "\n",
    "\n",
    "print('Creating residual cube, model cube and saving...')\n",
    "start_time = time.time()\n",
    "new_header = new_header_v_decomp(hdu,len(stack_model[0,0][0]),val_type='Residuals',val_unit='km/s')\n",
    "new_cube = sc.SpectralCube(data=final_residuals,header=original_header,wcs=w)\n",
    "new_cube.write(f'V_decomp//C18O//{species}_residuals.{chan_num}.cube.fits',format='fits',overwrite=True)\n",
    "\n",
    "# Need to mask the total model as anything outside of the observed field is also set to 0\n",
    "mask = np.isfinite(final_residuals)\n",
    "new_cube = sc.SpectralCube(data=final_model,header=original_header,wcs=w)\n",
    "new_cube = new_cube.with_mask(mask)\n",
    "new_cube.write(f'V_decomp//C18O//{species}_total_model.{chan_num}.cube.fits',format='fits',overwrite=True)\n",
    "\n",
    "# Save the non summed model. Mainly for debugigng to see which gaussians were used to create each point of the model\n",
    "np.save('V_decomp//C18O//non_summed-model',model)\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "\n",
    "\n",
    "print('Saving number of gaussian and threshold plot...')\n",
    "start_time = time.time()\n",
    "mask_single = np.isfinite(final_residuals[0,:,:])\n",
    "fit_count_masked = np.where(mask_single==True,fit_count,np.nan)\n",
    "fit_map_fits = fits.PrimaryHDU(data=fit_count_masked)\n",
    "fit_map_fits.writeto(f'V_decomp//C18O//{species}_fit_count.{chan_num}.fits',overwrite=True)\n",
    "\n",
    "trans = np.transpose(threshold,axes=[2,0,1])\n",
    "threshold_map = fits.PrimaryHDU(data=trans)\n",
    "threshold_map.writeto(f'V_decomp//C18O//{species}_threshold.{chan_num}.fits',overwrite=True)\n",
    "print(f\"Execution time: { (time.time() - start_time) /60:.2f} minutes\")\n",
    "print(\"Done :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Make optmize warning into errors (i.e. covariance can\\'t be estimated so it is a poorfit\\nimport warnings\\nwarnings.filterwarnings(\\'error\\',\\'Covariance of the parameters could not be estimated\\',scipy.optimize._optimize.OptimizeWarning)\\ndef gaussian(velocity,A,v_0, sigma):\\n    return (A * np.exp( - (velocity-v_0)**2 / (2*sigma**2)))\\n\\n\\ndef gaussian_to_max(ra,dec,current_slice,index=0):\\n    # Find V_rad channel at max T\\n    try:\\n        channel_max = np.nanargmax(current_slice)\\n    except ValueError:\\n       # print(f\"Ra index: {ra}, Dec index: {dec} is all Nans. Skipping...\")\\n        return (np.nan,np.nan,np.nan)\\n    \\n    if channel_max <= 10:\\n        print(f\"Max before channel 10: Ra = {ra}, Dec = {dec} - Thread {index}\")\\n        return (np.nan,np.nan,np.nan)\\n    # Fit gaussian to channel\\n    try:\\n        velocity_current_cont = velocity_cont[channel_max-10:channel_max+10]\\n        selected_current_slice = current_slice[channel_max-10:channel_max+10]\\n        \\n        if (len(np.isfinite(selected_current_slice))<=4):\\n            print(f\"Fewer than 5 data points: Ra = {ra}, Dec = {dec} - Thread {index}\")\\n            return (np.nan,np.nan,np.nan)\\n\\n        pop, pcov = curve_fit(gaussian,xdata=velocity_current_cont,ydata=selected_current_slice,p0=[5, velocity_cont[channel_max],20],nan_policy=\\'omit\\',bounds=([0,-85,0],[1000,95,100]))\\n    except RuntimeError:\\n        print(f\"Runtime error: Ra = {ra}, Dec = {dec} - Thread {index}\")\\n        return (np.nan,np.nan,np.nan)\\n    except IndexError:\\n        print(f\"Index error: Ra = {ra}, Dec = {dec} - Thread {index}\")\\n        return (np.nan,np.nan,np.nan)\\n    except scipy.optimize._optimize.OptimizeWarning:\\n        print(f\"Inditerminate covariance: Ra = {ra}, Dec = {dec} - Thread {index}\")\\n        return (np.nan,np.nan,np.nan)\\n    \\n    # Construct continuus gaussian and subtract from data \\n    fitted_gauss = gaussian(velocity_cont,pop[0],pop[1],pop[2])\\n    residuals = current_slice - fitted_gauss\\n    errors = np.sqrt(np.diag(pcov))\\n    return (residuals,pop,errors)\\n    \\n    # Chat GPT version, lists instead of arrays to solve appending issue\\n\\ndef process_chunk(data, index=0, dec_start_local=0):\\n    # Initialize lists instead of arrays\\n    residuals_local = []\\n    fit_params_local = []\\n    fit_errors_local = []\\n    \\n    shape = np.shape(data)\\n    ra_len = shape[2]\\n    dec_len = shape[1]\\n    \\n    for ra in range(ra_len):\\n        dec_pixel = dec_start_local\\n        print(f\\'Processing RA: {ra}/{ra_len}, dec length: {dec_len} - Thread {index}\\')\\n        \\n        for dec in range(dec_len):\\n            \\n            for i in range(2):\\n                if i == 0:\\n                    current_slice = data[:, dec, ra]\\n                    current_residuals = np.zeros_like(current_slice)\\n                else:\\n                    current_slice = current_residuals\\n                    \\n                current_slice = np.where(current_slice >= threshold, current_slice, np.nan)\\n                current_residuals, fit_param, fit_error = gaussian_to_max(ra + 1, dec_pixel + 1, current_slice, index)\\n                \\n                if np.isnan(current_residuals).all():\\n                    current_residuals = current_slice\\n                    fit_param, fit_error = (np.nan, np.nan, np.nan), (np.nan, np.nan, np.nan)\\n                else:\\n                    print(f\"Ra pixel: {ra + 1}, Dec pixel {dec_pixel + 1} worked. V_0 = {fit_param[1]} - Thread {index}\")\\n                \\n                # Append data to lists\\n                residuals_local.append(current_residuals)\\n                fit_params_local.append(fit_param)\\n                fit_errors_local.append(fit_error)\\n                \\n            dec_pixel += 1\\n        \\n    print(f\"Thread {index} finished :)\")\\n    \\n    # Convert lists to arrays\\n    residuals_local = np.array(residuals_local)\\n    fit_params_local = np.array(fit_params_local)\\n    fit_errors_local = np.array(fit_errors_local)\\n    \\n    np.save(f\\'output/residuals_chunk_{index}\\',residuals_local)\\n    np.save(f\\'output/fitparams_chunk_{index}\\',fit_params_local)\\n    np.save(f\\'output/fiterrors_chunk_{index}\\',fit_errors_local)\\n    \\n    return 0\\n\\nchunks = np.array_split(data,6,axis=1)\\nprint(np.shape(chunks[0]))\\n\\nra_start, dec_start = [],[]\\nprevious_dec_start = 0\\nfor i in range(6):\\n    if i!= 0:\\n        shape_current = np.shape(chunks[i])\\n        current_dec_start = shape_current[1]\\n        dec_start = np.append(dec_start,current_dec_start+previous_dec_start)\\n        \\n        previous_dec_start = current_dec_start + previous_dec_start\\n        \\n    else:\\n        dec_start = np.append(dec_start,0)\\n\\n#thread_1 = threading.Thread(target=process_chunk, args = (chunks[0],1,dec_start[0]))\\n#thread_2 = threading.Thread(target=process_chunk, args = (chunks[1],2,dec_start[1]))\\n#thread_3 = threading.Thread(target=process_chunk, args = (chunks[2],3,dec_start[2]))\\n#thread_4 = threading.Thread(target=process_chunk, args = (chunks[3],4,dec_start[3]))\\n#thread_5 = threading.Thread(target=process_chunk, args = (chunks[4],5,dec_start[4]))\\n#thread_6 = threading.Thread(target=process_chunk, args = (chunks[5],6,dec_start[5]))\\n\\n\\n#thread_1_vals = thread_1.start()\\n#thread_2_vals = thread_2.start()\\n#thread_3_vals = thread_3.start()\\n#thread_4_vals = thread_4.start()\\n#thread_5_vals = thread_5.start()\\n#thread_6_vals = thread_6.start()  '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old v_decomp version based on multi-threading\n",
    "# Not with the other due to it being a .py so no cells\n",
    "'''\n",
    "# Make optmize warning into errors (i.e. covariance can't be estimated so it is a poorfit\n",
    "import warnings\n",
    "warnings.filterwarnings('error','Covariance of the parameters could not be estimated',scipy.optimize._optimize.OptimizeWarning)\n",
    "def gaussian(velocity,A,v_0, sigma):\n",
    "    return (A * np.exp( - (velocity-v_0)**2 / (2*sigma**2)))\n",
    "\n",
    "\n",
    "def gaussian_to_max(ra,dec,current_slice,index=0):\n",
    "    # Find V_rad channel at max T\n",
    "    try:\n",
    "        channel_max = np.nanargmax(current_slice)\n",
    "    except ValueError:\n",
    "       # print(f\"Ra index: {ra}, Dec index: {dec} is all Nans. Skipping...\")\n",
    "        return (np.nan,np.nan,np.nan)\n",
    "    \n",
    "    if channel_max <= 10:\n",
    "        print(f\"Max before channel 10: Ra = {ra}, Dec = {dec} - Thread {index}\")\n",
    "        return (np.nan,np.nan,np.nan)\n",
    "    # Fit gaussian to channel\n",
    "    try:\n",
    "        velocity_current_cont = velocity_cont[channel_max-10:channel_max+10]\n",
    "        selected_current_slice = current_slice[channel_max-10:channel_max+10]\n",
    "        \n",
    "        if (len(np.isfinite(selected_current_slice))<=4):\n",
    "            print(f\"Fewer than 5 data points: Ra = {ra}, Dec = {dec} - Thread {index}\")\n",
    "            return (np.nan,np.nan,np.nan)\n",
    "\n",
    "        pop, pcov = curve_fit(gaussian,xdata=velocity_current_cont,ydata=selected_current_slice,p0=[5, velocity_cont[channel_max],20],nan_policy='omit',bounds=([0,-85,0],[1000,95,100]))\n",
    "    except RuntimeError:\n",
    "        print(f\"Runtime error: Ra = {ra}, Dec = {dec} - Thread {index}\")\n",
    "        return (np.nan,np.nan,np.nan)\n",
    "    except IndexError:\n",
    "        print(f\"Index error: Ra = {ra}, Dec = {dec} - Thread {index}\")\n",
    "        return (np.nan,np.nan,np.nan)\n",
    "    except scipy.optimize._optimize.OptimizeWarning:\n",
    "        print(f\"Inditerminate covariance: Ra = {ra}, Dec = {dec} - Thread {index}\")\n",
    "        return (np.nan,np.nan,np.nan)\n",
    "    \n",
    "    # Construct continuus gaussian and subtract from data \n",
    "    fitted_gauss = gaussian(velocity_cont,pop[0],pop[1],pop[2])\n",
    "    residuals = current_slice - fitted_gauss\n",
    "    errors = np.sqrt(np.diag(pcov))\n",
    "    return (residuals,pop,errors)\n",
    "    \n",
    "    # Chat GPT version, lists instead of arrays to solve appending issue\n",
    "\n",
    "def process_chunk(data, index=0, dec_start_local=0):\n",
    "    # Initialize lists instead of arrays\n",
    "    residuals_local = []\n",
    "    fit_params_local = []\n",
    "    fit_errors_local = []\n",
    "    \n",
    "    shape = np.shape(data)\n",
    "    ra_len = shape[2]\n",
    "    dec_len = shape[1]\n",
    "    \n",
    "    for ra in range(ra_len):\n",
    "        dec_pixel = dec_start_local\n",
    "        print(f'Processing RA: {ra}/{ra_len}, dec length: {dec_len} - Thread {index}')\n",
    "        \n",
    "        for dec in range(dec_len):\n",
    "            \n",
    "            for i in range(2):\n",
    "                if i == 0:\n",
    "                    current_slice = data[:, dec, ra]\n",
    "                    current_residuals = np.zeros_like(current_slice)\n",
    "                else:\n",
    "                    current_slice = current_residuals\n",
    "                    \n",
    "                current_slice = np.where(current_slice >= threshold, current_slice, np.nan)\n",
    "                current_residuals, fit_param, fit_error = gaussian_to_max(ra + 1, dec_pixel + 1, current_slice, index)\n",
    "                \n",
    "                if np.isnan(current_residuals).all():\n",
    "                    current_residuals = current_slice\n",
    "                    fit_param, fit_error = (np.nan, np.nan, np.nan), (np.nan, np.nan, np.nan)\n",
    "                else:\n",
    "                    print(f\"Ra pixel: {ra + 1}, Dec pixel {dec_pixel + 1} worked. V_0 = {fit_param[1]} - Thread {index}\")\n",
    "                \n",
    "                # Append data to lists\n",
    "                residuals_local.append(current_residuals)\n",
    "                fit_params_local.append(fit_param)\n",
    "                fit_errors_local.append(fit_error)\n",
    "                \n",
    "            dec_pixel += 1\n",
    "        \n",
    "    print(f\"Thread {index} finished :)\")\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    residuals_local = np.array(residuals_local)\n",
    "    fit_params_local = np.array(fit_params_local)\n",
    "    fit_errors_local = np.array(fit_errors_local)\n",
    "    \n",
    "    np.save(f'output/residuals_chunk_{index}',residuals_local)\n",
    "    np.save(f'output/fitparams_chunk_{index}',fit_params_local)\n",
    "    np.save(f'output/fiterrors_chunk_{index}',fit_errors_local)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "chunks = np.array_split(data,6,axis=1)\n",
    "print(np.shape(chunks[0]))\n",
    "\n",
    "ra_start, dec_start = [],[]\n",
    "previous_dec_start = 0\n",
    "for i in range(6):\n",
    "    if i!= 0:\n",
    "        shape_current = np.shape(chunks[i])\n",
    "        current_dec_start = shape_current[1]\n",
    "        dec_start = np.append(dec_start,current_dec_start+previous_dec_start)\n",
    "        \n",
    "        previous_dec_start = current_dec_start + previous_dec_start\n",
    "        \n",
    "    else:\n",
    "        dec_start = np.append(dec_start,0)\n",
    "\n",
    "#thread_1 = threading.Thread(target=process_chunk, args = (chunks[0],1,dec_start[0]))\n",
    "#thread_2 = threading.Thread(target=process_chunk, args = (chunks[1],2,dec_start[1]))\n",
    "#thread_3 = threading.Thread(target=process_chunk, args = (chunks[2],3,dec_start[2]))\n",
    "#thread_4 = threading.Thread(target=process_chunk, args = (chunks[3],4,dec_start[3]))\n",
    "#thread_5 = threading.Thread(target=process_chunk, args = (chunks[4],5,dec_start[4]))\n",
    "#thread_6 = threading.Thread(target=process_chunk, args = (chunks[5],6,dec_start[5]))\n",
    "\n",
    "\n",
    "#thread_1_vals = thread_1.start()\n",
    "#thread_2_vals = thread_2.start()\n",
    "#thread_3_vals = thread_3.start()\n",
    "#thread_4_vals = thread_4.start()\n",
    "#thread_5_vals = thread_5.start()\n",
    "#thread_6_vals = thread_6.start()  '''  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
